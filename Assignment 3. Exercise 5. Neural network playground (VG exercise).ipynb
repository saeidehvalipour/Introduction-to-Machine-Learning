{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3.Exercise 5: Neural network playground (VG exercise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Exercise 5 I decide to use the handwriting digits in the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import warnings; warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First I use the load_digits [1] convenience function in official sklearn documentation to load the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = digits.data\n",
    "y = digits.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network depth and Layer width "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For fully connected neural networks, there are three essential criteria that define the network’s architecture:\n",
    "\n",
    "How many layers are there?\n",
    "How many nodes are there in each of those layers?\n",
    "What transfer/activation function is used at each of those layers?\n",
    "\n",
    "There are other factors that can contribute to the performance of a neural network. These include which loss function is used, which optimization algorithm is used and etc. I’ve selected a common loss function called categorical cross entropy. plus, I’ve selected one of the simplest optimization algorithms: Stochastic Gradient Descent (SGD).\n",
    "\n",
    "MLPClassifier on sklearn trains using Backpropagation. More precisely, it trains using some form of gradient descent and the gradients are calculated using Backpropagation. For classification, it minimizes the Cross-Entropy loss function,\n",
    "\n",
    "I use MLPClassifier with three different sets of parameters for the neural network include 1. number of hidden layers 2. number of neurons per layer 3. activation functions.\n",
    "First I start by a Network contains an input layer, a hidden layer with 150 units, a hidden layer with 100 units, and an output layer. I also increased the value of the regularization hyperparameter alpha argument. Also, I used 'lbfgs' as an optimizer in the family of quasi-Newton methods.\n",
    "Finally I print the accuracies of the three cross validation fold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. First Set: 'number of hidden layers': 150,'number of neurons per layer': 100,'activation':logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1 = MLPClassifier(activation='logistic',\n",
    "max_iter=1000000, hidden_layer_sizes=(150,100),alpha=0.1, solver='lbfgs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='logistic', alpha=0.1, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(150, 100), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=1000000, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=None, shuffle=True, solver='lbfgs', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf1.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of MLP weights on MNIST "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've tried to Visualization of MLPClassifier weights on MNIST using code in sklearn documentation [2]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, axes = plt.subplots(4, 4)\n",
    "# use global min / max to ensure all weights are shown on the same scale\n",
    "vmin, vmax = clf1.coefs_[0].min(), clf1.coefs_[0].max()\n",
    "for coef, ax in zip(clf1.coefs_[0].T, axes.ravel()):\n",
    "    ax.matshow(coef.reshape(8, 8), cmap=plt.cm.gray, vmin=.5 * vmin,\n",
    "               vmax=.5 * vmax)\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Layers: 4, Number of outputs: 10, loss function: 0.012198055135810359\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "print('Number of Layers: %s, Number of outputs: %s, loss function: %s' % (clf1.n_layers_, clf1.n_outputs_, clf1.loss_))\n",
    "predictions = clf1.score(X, y)\n",
    "print('Accuracy:', clf1.score(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.95182724 0.96661102 0.94463087]\n"
     ]
    }
   ],
   "source": [
    "print(cross_val_score(clf1, X, y, n_jobs = -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We defined the input layer earlier as a vector with 784 entries, this is the data from the flattened 28x28 image. \n",
    "The second model has a single hidden layer with 32 nodes using the sigmoid activation function. The resulting architecture has 25,450 tunable parameters. From the input layer to the hidden layer there are 784*32 = 25,088 weights. The hidden layer has 32 nodes so there are 32 biases. This brings us to 25,088 + 32 = 25,120 parameters.\n",
    "\n",
    "The network below has one hidden layer. This network is so shallow that it’s technically inaccurate to call it “deep learning”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Second Set: 'number of hidden layers': 1, 'number of neurons per layer': 32, 'activation':relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2 = MLPClassifier(activation='relu',\n",
    "max_iter=10000, hidden_layer_sizes=(32,),alpha=0.1, solver='sgd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.1, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(32,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=10000, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=None, shuffle=True, solver='sgd', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf2.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Layers: 3, Number of outputs: 10, loss function: 0.0324339470102941\n",
      "Accuracy: 0.998330550918197\n"
     ]
    }
   ],
   "source": [
    "print('Number of Layers: %s, Number of outputs: %s, loss function: %s' % (clf2.n_layers_, clf2.n_outputs_, clf2.loss_))\n",
    "predictions = clf2.score(X, y)\n",
    "print('Accuracy:', clf2.score(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.92857143 0.92320534 0.91442953]\n"
     ]
    }
   ],
   "source": [
    "print(cross_val_score(clf2, X, y, n_jobs = -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Third Set: 'number of hidden layers': 100, 'number of neurons per layer': 200, 'activation':tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf3 = MLPClassifier(activation='tanh',\n",
    "max_iter=1000000, hidden_layer_sizes=(100,200),alpha=0.1, solver='lbfgs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='tanh', alpha=0.1, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100, 200), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=1000000, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=None, shuffle=True, solver='lbfgs', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf3.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Layers: 4, Number of outputs: 10,  loss function: 0.0030616733930209205\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "print('Number of Layers: %s, Number of outputs: %s,  loss function: %s' % (clf3.n_layers_, clf3.n_outputs_, clf3.loss_))\n",
    "predictions = clf3.score(X, y)\n",
    "print('Accuracy:', clf3.score(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.95016611 0.97161937 0.93791946]\n"
     ]
    }
   ],
   "source": [
    "print(cross_val_score(clf3, X, y, n_jobs = -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we train long enough and have \"too many\" hidden layer units, then we will eventually have over-training. Usually, the goal is to find the smallest number of hidden units that can do reasonably well, as that will generally give you good generalization. In fact, there is the \"bottle neck theory\" that attempts to show this effect.\n",
    "\n",
    "Adding more layers appears to have decreased the accuracy of the model. Overfitting is a problem with many machine learning tasks. Neural networks are especially prone to overfitting because of the very large number of tunable parameters. One sign that you might be overfitting is that the training accuracy is significantly better than the test accuracy.\n",
    "\n",
    "Another knob we can turn is the number of nodes in each hidden layer. This is called the width of the layer. As with adding more layers, making each layer wider increases the total number of tunable parameters. Making wider layers tends to scale the number of parameters faster than adding more layers. Every time we add a single node to layer i, we have to give that new node an edge to every node in layer i+1.\n",
    "\n",
    "Finally, because of the optimization algorithms work with neural networks, deeper networks require more training time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[2] https://scikit-learn.org/stable/auto_examples/neural_networks/plot_mnist_filters.html#sphx-glr-auto-examples-neural-networks-plot-mnist-filters-py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
